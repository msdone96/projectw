# File: projectw/.github/workflows/scraper.yml

name: WhatsApp Link Scraper
on:
  workflow_dispatch:
    inputs:
      batch_size:
        description: 'Number of URLs to process in parallel'
        required: true
        default: '50'
      max_retries:
        description: 'Maximum retry attempts per URL'
        required: true
        default: '3'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 350  # Just under 6 hours

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Create directories
      run: |
        mkdir -p output

    - name: Download previous results
      uses: actions/download-artifact@v4
      with:
        name: scraping-results
        path: output/
      continue-on-error: true

    - name: Run scraper
      run: |
        python src/scraper.py \
          --input input/gplinks.xlsx \
          --output-dir output \
          --batch-size ${{ github.event.inputs.batch_size }} \
          --max-retries ${{ github.event.inputs.max_retries }}
      env:
        PYTHONUNBUFFERED: 1

    - name: Prepare results
      if: always()
      run: |
        tar -czf scraping-results.tar.gz output/
        echo "### Scraping Summary" > summary.md
        if [ -f output/scraper.log ]; then
          tail -n 10 output/scraper.log >> summary.md
        fi

    - name: Upload results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraping-results
        path: |
          scraping-results.tar.gz
          summary.md
        retention-days: 30

    - name: Process status
      if: always()
      run: |
        echo "### Results Files"
        ls -lh output/
        
        echo "### Recent Logs"
        if [ -f output/scraper.log ]; then
          tail output/scraper.log
        fi
